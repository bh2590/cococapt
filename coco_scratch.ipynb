{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(10, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(inputs[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp= torch.stack(inputs, dim=0)\n",
    "inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.randn(1, 1, 3).repeat(1,10,1), torch.randn(1, 1, 3).repeat(1,10,1))\n",
    "out, fstate= lstm(inp, hidden)\n",
    "print(out.size())\n",
    "print(fstate.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(torch.tensor([[1,3], [3,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "type(torch.tensor(np.array([[1,3], [3,0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "cap = dset.CocoCaptions(root = '/Users/hanozbhathena/Documents/coco/data/val2017',\n",
    "                        annFile = '/Users/hanozbhathena/Documents/coco/data/annotations/captions_val2017.json',\n",
    "                        transform= data_transform)\n",
    "\n",
    "print('Number of samples: ', len(cap))\n",
    "img, target = cap[3] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.size())\n",
    "print(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= resnet18(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.maxpool(x)\n",
    "    return x\n",
    "\n",
    "out2= forward(resnet18, img.unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "caption_dset = dset.CocoCaptions(root = '/Users/hanozbhathena/Documents/coco/data/val2017',\n",
    "                        annFile = '/Users/hanozbhathena/Documents/coco/data/annotations/captions_val2017.json',\n",
    "                        transform= data_transform)\n",
    "\n",
    "words_set= set()\n",
    "for i in tqdm(range(len(caption_dset))):\n",
    "    _, caption= caption_dset[i]\n",
    "    tokens= set(word_tokenize(' '.join(caption).lower()))\n",
    "    words_set= words_set.union(tokens)\n",
    "\n",
    "word_list= list(words_set)\n",
    "word_to_idx= dict(zip(word_list, range(len(word_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "from ipdb import slaunch_ipdb_on_exception\n",
    "filepath=\"/Users/hanozbhathena/Documents/coco/data/glove.840B.300d.txt\"\n",
    "dic = {}\n",
    "words= []\n",
    "with open(filepath) as ifs:\n",
    "    for line in ifs:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        row = line.split()\n",
    "        token = row[0]\n",
    "        if token not in tokens:\n",
    "            continue\n",
    "        try:\n",
    "            data = [float(x) for x in row[1:]]\n",
    "        except ValueError:\n",
    "            words.append(' '.join(row[:-300]))\n",
    "            continue\n",
    "        if len(data) != dimensions:\n",
    "            raise RuntimeError(\"wrong number of dimensions\")\n",
    "        dic[token] = np.asarray(data)\n",
    "print(\"{} lines were not meeting split standards: {}\".format(len(words), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_caption_word_dict(root = '/Users/hanozbhathena/Documents/coco/data/val2017',\n",
    "                           annFile = '/Users/hanozbhathena/Documents/coco/data/annotations/captions_val2017.json'):\n",
    "    data_transform = transforms.Compose([\n",
    "            transforms.RandomSizedCrop(224),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            #                     std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    caption_dset = dset.CocoCaptions(root= root, annFile= annFile,\n",
    "                                     transform= data_transform)\n",
    "    \n",
    "    words_set= set()\n",
    "    for i in tqdm(range(len(caption_dset))):\n",
    "        _, caption= caption_dset[i]\n",
    "        tokens= set(word_tokenize(' '.join(caption).lower()))\n",
    "        words_set= words_set.union(tokens)\n",
    "    \n",
    "    word_list= list(words_set)\n",
    "    word_to_idx= dict(zip(word_list, range(len(word_list))))\n",
    "    return word_to_idx\n",
    "\n",
    "def loadWordVectors(tokens, filepath=\"/Users/hanozbhathena/Documents/coco/data/glove.840B.300d.txt\", dimensions=300):\n",
    "    \"\"\"Read pretrained GloVe vectors\"\"\"\n",
    "    wordVectors = np.zeros((len(tokens), dimensions))\n",
    "    dic = {}\n",
    "    words= []\n",
    "    with open(filepath) as ifs:\n",
    "        for line in ifs:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            row = line.split()\n",
    "            token = row[0]\n",
    "            if token not in tokens:\n",
    "                continue\n",
    "            try:\n",
    "                data = [float(x) for x in row[1:]]\n",
    "            except ValueError:\n",
    "                words.append(' '.join(row[:-300]))\n",
    "                continue\n",
    "            if len(data) != dimensions:\n",
    "                raise RuntimeError(\"wrong number of dimensions\")\n",
    "            dic[token] = np.asarray(data)\n",
    "    logging.info(\"{} lines were not meeting split standards: {}\".format(len(words), words))\n",
    "    dic[SpecialTokens.START]= np.random.uniform(low=-0.25, high=0.25, size=dimensions)\n",
    "    dic[SpecialTokens.END]= np.random.uniform(low=-0.25, high=0.25, size=dimensions)\n",
    "    dic[SpecialTokens.OOV]= np.random.uniform(low=-0.25, high=0.25, size=dimensions)\n",
    "    dic[SpecialTokens.PAD]= np.random.uniform(low=-0.25, high=0.25, size=dimensions)\n",
    "    \n",
    "    tokens[SpecialTokens.START]= len(tokens)\n",
    "    tokens[SpecialTokens.END]= len(tokens)\n",
    "    tokens[SpecialTokens.OOV]= len(tokens)\n",
    "    tokens[SpecialTokens.PAD]= len(tokens)\n",
    "    \n",
    "    oov_count= 0\n",
    "    for word, ind in tokens.items():\n",
    "        try:\n",
    "            wordVectors[ind]= dic[word]\n",
    "        except KeyError:\n",
    "            oov_count+=1\n",
    "            wordVectors[ind]= dic[SpecialTokens.OOV]\n",
    "    logging.info(\"{} words from COCO were OOV for Glove\".format(oov_count))\n",
    "    logging.info(\"word vectors loaded using\" + filepath)\n",
    "    return wordVectors, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def clean_text(text):\n",
    "    #Logic to clean text: review from Keras tutorial or some COCO captions tutorial online\n",
    "    return text\n",
    "\n",
    "class MyCOCODset(Dataset):\n",
    "    def __init__(self, coco_dset, word_to_idx, max_seq_len, pad_symbol):\n",
    "        self.coco_dset= coco_dset\n",
    "        self.word_to_idx= word_to_idx\n",
    "        self.max_seq_len= max_seq_len\n",
    "        self.pad_symbol= pad_symbol\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        img, str_caption= self.coco_dset[ind]\n",
    "        str_caption= str_caption[np.random.randint(len(str_caption))].lower()\n",
    "        ind_caption= [self.word_to_idx[w] for w in word_tokenize(clean_text(str_caption))]\n",
    "        real_len= len(ind_caption)\n",
    "        ind_caption= ind_caption + [self.pad_symbol] * (self.max_seq_len - real_len)\n",
    "        return img, torch.tensor(ind_caption, dtype= torch.long), real_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coco_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dset= MyCOCODset(caption_dset, word_to_idx, 30, 10000)\n",
    "img, capt, l= my_dset[np.random.randint(1000)]\n",
    "print(img.size())\n",
    "print(capt, len(capt))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "x = Variable(torch.randn(10, 20, 30))\n",
    "lens = range(10)\n",
    "\n",
    "x = pack_padded_sequence(x, lens[::-1], batch_first=True)\n",
    "\n",
    "lstm = nn.LSTM(30, 50, batch_first=True)\n",
    "h0 = Variable(torch.zeros(1, 10, 50))\n",
    "c0 = Variable(torch.zeros(1, 10, 50))\n",
    "\n",
    "packed_h, (packed_h_t, packed_c_t) = lstm(x, (h0, c0))\n",
    "h, _ = pad_packed_sequence(packed_h) \n",
    "print h.size() # Size 20 x 10 x 50 instead of 10 x 20 x 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(10, 20, 30))\n",
    "lens = list(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = pack_padded_sequence(x, lens[::-1], batch_first=True)\n",
    "lstm = nn.LSTM(30, 50, batch_first=True)\n",
    "h0 = Variable(torch.zeros(1, 10, 50))\n",
    "c0 = Variable(torch.zeros(1, 10, 50))\n",
    "\n",
    "packed_h, (packed_h_t, packed_c_t) = lstm(x2, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, _ = pad_packed_sequence(packed_h) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "# make it easier to create the sequences of vectors\n",
    "embs1=torch.nn.Embedding(100,3,padding_idx=0)\n",
    "lstm1 = torch.nn.LSTM(3,5,1)  # lst takes 3-dimensional inputs\n",
    "batch= torch.tensor([[1,2,0,0,0,0],[3,0,0,0,0,0],[2,4,5,2,3,1],[4,1,2,2,0,0]], dtype= torch.long)\n",
    "e_batch = embs1(batch)   # get the batch of sequences of embeddings\n",
    "# Note: the batch is of shape batchsize,maxseq,3 so we need to use batch_first later\n",
    "# these are my sequence lengths\n",
    "lens = [2,1,6,4]\n",
    "lens_sorted,idx = torch.IntTensor(lens).sort(0, descending=True)\n",
    "# sort the embeddings batch by lengths\n",
    "e_batch_sorted = e_batch[idx]\n",
    "# create the packed sequences\n",
    "packed=torch.nn.utils.rnn.pack_padded_sequence(e_batch_sorted, lens_sorted.tolist(), batch_first=True)\n",
    "# get the output from the lstm\n",
    "(out,(hout,cout))=lstm1(packed)\n",
    "(unpacked_out,_) = torch.nn.utils.rnn.pad_packed_sequence(out,batch_first=True)\n",
    "# would like to unsort like so:\n",
    "_,orig_idx = idx.sort(0)\n",
    "# original, attempt which did not work\n",
    "# unsort_idx = orig_idx.view(-1,1,1).expand_as(unpacked_out)\n",
    "# unsorted = unpacked_out.gather(0,unsort_idx.long())\n",
    "\n",
    "# UPDATE: this seems to do what I want:\n",
    "unsorted = unpacked_out[orig_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs1=torch.nn.Embedding(100,3,padding_idx=0)\n",
    "lstm1 = torch.nn.LSTM(3,5,1)  # lst takes 3-dimensional inputs\n",
    "batch= torch.tensor([[1,2,0,0,0,0],[3,0,0,0,0,0],[2,4,5,2,3,1],[4,1,2,2,0,0]], dtype= torch.long)\n",
    "lens = [2,1,6,4]\n",
    "lens_sorted,idx = torch.IntTensor(lens).sort(0, descending=True)\n",
    "e_batch = embs1(batch)\n",
    "# sort the embeddings batch by lengths\n",
    "e_batch_sorted = e_batch[idx]\n",
    "# create the packed sequences\n",
    "packed=torch.nn.utils.rnn.pack_padded_sequence(e_batch_sorted, lens_sorted.tolist(), batch_first=True)\n",
    "# get the output from the lstm\n",
    "(out,(hout,cout))=lstm1(packed)\n",
    "(unpacked_out,_) = torch.nn.utils.rnn.pad_packed_sequence(out,batch_first=True)\n",
    "\n",
    "# would like to unsort like so:\n",
    "_,orig_idx = idx.sort(0)\n",
    "\n",
    "# UPDATE: this seems to do what I want:\n",
    "unsorted = unpacked_out[orig_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch, lens_sorted, idx, orig_idx, e_batch_sorted, packed, out, unpacked_out, unsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embed_size, output_size):\n",
    "        super(mLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # input embedding\n",
    "        self.encoder = nn.Embedding(input_size, embed_size)\n",
    "        # lstm weights\n",
    "        self.weight_fm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_im = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_cm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_om = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_fx = nn.Linear(embed_size, hidden_size)\n",
    "        self.weight_ix = nn.Linear(embed_size, hidden_size)\n",
    "        self.weight_cx = nn.Linear(embed_size, hidden_size)\n",
    "        self.weight_ox = nn.Linear(embed_size, hidden_size)\n",
    "        # multiplicative weights\n",
    "        self.weight_mh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.weight_mx = nn.Linear(embed_size, hidden_size)\n",
    "        # decoder\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, inp, h_0, c_0):\n",
    "        # encode the input characters\n",
    "        inp = self.encoder(inp)\n",
    "        # calculate the multiplicative matrix\n",
    "        m_t = self.weight_mx(inp) * self.weight_mh(h_0)\n",
    "        # forget gate\n",
    "        f_g = F.sigmoid(self.weight_fx(inp) + self.weight_fm(m_t))\n",
    "        # input gate\n",
    "        i_g = F.sigmoid(self.weight_ix(inp) + self.weight_im(m_t))\n",
    "        # output gate\n",
    "        o_g = F.sigmoid(self.weight_ox(inp) + self.weight_om(m_t))\n",
    "        # intermediate cell state\n",
    "        c_tilda = F.tanh(self.weight_cx(inp) + self.weight_cm(m_t))\n",
    "        # current cell state\n",
    "        cx = f_g * c_0 + i_g * c_tilda\n",
    "        # hidden state\n",
    "        hx = o_g * F.tanh(cx)\n",
    "\n",
    "        out = self.decoder(hx.view(1,-1))\n",
    "\n",
    "        return out, hx, cx\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h_0 = Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "        c_0 = Variable(torch.zeros(1, self.hidden_size)).cuda()\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img= img.numpy()\n",
    "np_img= np.squeeze(np_img, axis= 0)\n",
    "np_img= np_img.transpose(1, 2, 0)\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.imshow(np_img, interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "from PIL import Image\n",
    "img = Image.fromarray(np_img, 'RGB')\n",
    "img.save('my.png')\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "(ToPILImage(img.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
