{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pdb\n",
    "\n",
    "logger = logging.getLogger(\"Data preprocessing\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCaptions_Cust(Dataset):\n",
    "    def __init__(self, root, annFile, transform=None, target_transform=None):\n",
    "        from pycocotools.coco import COCO\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is a list of captions for the image.\n",
    "        \"\"\"\n",
    "        #pdb.set_trace()\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        target = [ann['caption'] for ann in anns]\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, img_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "#download pre-trained models\n",
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet101 = models.resnet101(pretrained=True)\n",
    "resnet152 = models.resnet152(pretrained=True)\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "densenet = models.densenet161(pretrained=True)\n",
    "inception = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model= resnet18\n",
    "model_str= 'resnet18'\n",
    "model= model.to(device)\n",
    "path= '/home/hanozbhathena/project/data/img_features/' + model_str\n",
    "file= model_str + '_img_distr.pickle'\n",
    "train_outfilename= os.path.join(path, 'train_' + file)\n",
    "val_outfilename= os.path.join(path, 'val_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE= 100\n",
    "NUM_WORKERS= 8\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "train_caption_dset = CocoCaptions_Cust(root= '/home/hanozbhathena/project/data/resizetrain2017', \n",
    "                                 annFile= '/home/hanozbhathena/project/data/annotations/captions_train2017.json',\n",
    "                                 transform= data_transform)\n",
    "\n",
    "train_dataloader= DataLoader(train_caption_dset, batch_size= BATCH_SIZE,\n",
    "                               shuffle=False, num_workers= NUM_WORKERS)\n",
    "\n",
    "val_caption_dset = CocoCaptions_Cust(root= '/home/hanozbhathena/project/data/resizeval2017', \n",
    "                                 annFile= '/home/hanozbhathena/project/data/annotations/captions_val2017.json',\n",
    "                                 transform= data_transform)\n",
    "\n",
    "val_dataloader= DataLoader(val_caption_dset, batch_size= BATCH_SIZE,\n",
    "                           shuffle=False, num_workers= NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _, _= train_caption_dset[10]\n",
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_wo_last_n_layers(model, n= 1)\n",
    "    layers= []\n",
    "    for name, layer in model._modules.items():\n",
    "        layers.append(layer)\n",
    "    return_model= torch.nn.Sequential(*layers[:-n])\n",
    "    return return_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-45976ee6f763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36madd_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             raise TypeError(\"{} is not a Module subclass\".format(\n\u001b[0;32m--> 165\u001b[0;31m                 torch.typename(module)))\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute '{}' already exists\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple is not a Module subclass"
     ]
    }
   ],
   "source": [
    "print(torch.nn.Sequential(*list(resnet18._modules.items())[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Finished 1000/118287\n",
      "INFO:Finished 2000/118287\n",
      "INFO:Finished 3000/118287\n",
      "INFO:Finished 4000/118287\n",
      "INFO:Finished 5000/118287\n",
      "INFO:Finished 6000/118287\n",
      "INFO:Finished 7000/118287\n",
      "INFO:Finished 8000/118287\n",
      "INFO:Finished 9000/118287\n",
      "INFO:Finished 10000/118287\n",
      "INFO:Finished 11000/118287\n",
      "INFO:Finished 12000/118287\n",
      "INFO:Finished 13000/118287\n",
      "INFO:Finished 14000/118287\n",
      "INFO:Finished 15000/118287\n",
      "INFO:Finished 16000/118287\n",
      "INFO:Finished 17000/118287\n",
      "INFO:Finished 18000/118287\n",
      "INFO:Finished 19000/118287\n",
      "INFO:Finished 20000/118287\n",
      "INFO:Finished 21000/118287\n",
      "INFO:Finished 22000/118287\n",
      "INFO:Finished 23000/118287\n",
      "INFO:Finished 24000/118287\n",
      "INFO:Finished 25000/118287\n",
      "INFO:Finished 26000/118287\n",
      "INFO:Finished 27000/118287\n",
      "INFO:Finished 28000/118287\n",
      "INFO:Finished 29000/118287\n",
      "INFO:Finished 30000/118287\n",
      "INFO:Finished 31000/118287\n",
      "INFO:Finished 32000/118287\n",
      "INFO:Finished 33000/118287\n",
      "INFO:Finished 34000/118287\n",
      "INFO:Finished 35000/118287\n",
      "INFO:Finished 36000/118287\n",
      "INFO:Finished 37000/118287\n",
      "INFO:Finished 38000/118287\n",
      "INFO:Finished 39000/118287\n",
      "INFO:Finished 40000/118287\n",
      "INFO:Finished 41000/118287\n",
      "INFO:Finished 42000/118287\n",
      "INFO:Finished 43000/118287\n",
      "INFO:Finished 44000/118287\n",
      "INFO:Finished 45000/118287\n",
      "INFO:Finished 46000/118287\n",
      "INFO:Finished 47000/118287\n",
      "INFO:Finished 48000/118287\n",
      "INFO:Finished 49000/118287\n",
      "INFO:Finished 50000/118287\n",
      "INFO:Finished 51000/118287\n",
      "INFO:Finished 52000/118287\n",
      "INFO:Finished 53000/118287\n",
      "INFO:Finished 54000/118287\n",
      "INFO:Finished 55000/118287\n",
      "INFO:Finished 56000/118287\n",
      "INFO:Finished 57000/118287\n",
      "INFO:Finished 58000/118287\n",
      "INFO:Finished 59000/118287\n",
      "INFO:Finished 60000/118287\n",
      "INFO:Finished 61000/118287\n",
      "INFO:Finished 62000/118287\n",
      "INFO:Finished 63000/118287\n",
      "INFO:Finished 64000/118287\n",
      "INFO:Finished 65000/118287\n",
      "INFO:Finished 66000/118287\n",
      "INFO:Finished 67000/118287\n",
      "INFO:Finished 68000/118287\n",
      "INFO:Finished 69000/118287\n",
      "INFO:Finished 70000/118287\n",
      "INFO:Finished 71000/118287\n",
      "INFO:Finished 72000/118287\n",
      "INFO:Finished 73000/118287\n",
      "INFO:Finished 74000/118287\n",
      "INFO:Finished 75000/118287\n",
      "INFO:Finished 76000/118287\n",
      "INFO:Finished 77000/118287\n",
      "INFO:Finished 78000/118287\n",
      "INFO:Finished 79000/118287\n",
      "INFO:Finished 80000/118287\n",
      "INFO:Finished 81000/118287\n",
      "INFO:Finished 82000/118287\n",
      "INFO:Finished 83000/118287\n",
      "INFO:Finished 84000/118287\n",
      "INFO:Finished 85000/118287\n",
      "INFO:Finished 86000/118287\n",
      "INFO:Finished 87000/118287\n",
      "INFO:Finished 88000/118287\n",
      "INFO:Finished 89000/118287\n",
      "INFO:Finished 90000/118287\n",
      "INFO:Finished 91000/118287\n",
      "INFO:Finished 92000/118287\n",
      "INFO:Finished 93000/118287\n",
      "INFO:Finished 94000/118287\n",
      "INFO:Finished 95000/118287\n",
      "INFO:Finished 96000/118287\n",
      "INFO:Finished 97000/118287\n",
      "INFO:Finished 98000/118287\n",
      "INFO:Finished 99000/118287\n",
      "INFO:Finished 100000/118287\n",
      "INFO:Finished 101000/118287\n",
      "INFO:Finished 102000/118287\n",
      "INFO:Finished 103000/118287\n",
      "INFO:Finished 104000/118287\n",
      "INFO:Finished 105000/118287\n",
      "INFO:Finished 106000/118287\n",
      "INFO:Finished 107000/118287\n",
      "INFO:Finished 108000/118287\n",
      "INFO:Finished 109000/118287\n",
      "INFO:Finished 110000/118287\n",
      "INFO:Finished 111000/118287\n",
      "INFO:Finished 112000/118287\n",
      "INFO:Finished 113000/118287\n",
      "INFO:Finished 114000/118287\n",
      "INFO:Finished 115000/118287\n",
      "INFO:Finished 116000/118287\n",
      "INFO:Finished 117000/118287\n",
      "INFO:Finished 118000/118287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118287, 1000)\n"
     ]
    }
   ],
   "source": [
    "img_class_distr= []\n",
    "with torch.no_grad():\n",
    "    for i, (img, _, idx) in enumerate(train_dataloader):\n",
    "        img= img.to(device)\n",
    "        distr= model(img).cpu().numpy()\n",
    "        img_class_distr.append(distr)\n",
    "        if i > 0 and i*BATCH_SIZE % 1000 == 0:\n",
    "            logging.info(\"Finished {}/{}\".format(i*BATCH_SIZE, len(train_caption_dset)))\n",
    "\n",
    "img_class_distr= np.concatenate(img_class_distr, axis=0)\n",
    "print(img_class_distr.shape)\n",
    "\n",
    "with open(train_outfilename, 'wb') as fo:\n",
    "    pickle.dump(img_class_distr, fo, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "img_class_distr= []\n",
    "with torch.no_grad():\n",
    "    for i, (img, _, idx) in enumerate(val_dataloader):\n",
    "        img= img.to(device)\n",
    "        distr= model(img).cpu().numpy()\n",
    "        img_class_distr.append(distr)\n",
    "        if i > 0 and i*BATCH_SIZE % 1000 == 0:\n",
    "            logging.info(\"Finished {}/{}\".format(i*BATCH_SIZE, len(val_caption_dset)))\n",
    "\n",
    "\n",
    "img_class_distr= np.concatenate(img_class_distr, axis=0)\n",
    "print(img_class_distr.shape)\n",
    "\n",
    "with open(val_outfilename, 'wb') as fo:\n",
    "    pickle.dump(img_class_distr, fo, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_models(model_dict, model_name):\n",
    "    print(\"Model is {}\".format(model_name))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model= model_dict[model_name]\n",
    "    model_str= model_name\n",
    "    model= model.to(device)\n",
    "    path= '/home/hanozbhathena/project/data/img_features/' + model_str\n",
    "    file= model_str + '_img_distr.pickle'\n",
    "    train_outfilename= os.path.join(path, 'train_' + file)\n",
    "    val_outfilename= os.path.join(path, 'val_' + file)    \n",
    "    \n",
    "    #Prepare the dataset class\n",
    "    BATCH_SIZE= 100\n",
    "    NUM_WORKERS= 8\n",
    "\n",
    "    data_transform = transforms.Compose([\n",
    "            transforms.RandomSizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    train_caption_dset = CocoCaptions_Cust(root= '/home/hanozbhathena/project/data/resizetrain2017', \n",
    "                                     annFile= '/home/hanozbhathena/project/data/annotations/captions_train2017.json',\n",
    "                                     transform= data_transform)\n",
    "\n",
    "    train_dataloader= DataLoader(train_caption_dset, batch_size= BATCH_SIZE,\n",
    "                                   shuffle=False, num_workers= NUM_WORKERS)\n",
    "\n",
    "    val_caption_dset = CocoCaptions_Cust(root= '/home/hanozbhathena/project/data/resizeval2017', \n",
    "                                     annFile= '/home/hanozbhathena/project/data/annotations/captions_val2017.json',\n",
    "                                     transform= data_transform)\n",
    "\n",
    "    val_dataloader= DataLoader(val_caption_dset, batch_size= BATCH_SIZE,\n",
    "                               shuffle=False, num_workers= NUM_WORKERS)\n",
    "\n",
    "\n",
    "    #Training set loop\n",
    "    img_class_distr= []\n",
    "    with torch.no_grad():\n",
    "        for i, (img, _, idx) in enumerate(train_dataloader):\n",
    "            img= img.to(device)\n",
    "            distr= model(img).cpu().numpy()\n",
    "            img_class_distr.append(distr)\n",
    "            if i > 0 and i*BATCH_SIZE % 1000 == 0:\n",
    "                logging.info(\"Train Finished {}/{}\".format(i*BATCH_SIZE, len(train_caption_dset)))\n",
    "\n",
    "    img_class_distr= np.concatenate(img_class_distr, axis=0)\n",
    "    print(img_class_distr.shape)\n",
    "\n",
    "    with open(train_outfilename, 'wb') as fo:\n",
    "        pickle.dump(img_class_distr, fo, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #Dev set loop\n",
    "    img_class_distr= []\n",
    "    with torch.no_grad():\n",
    "        for i, (img, _, idx) in enumerate(val_dataloader):\n",
    "            img= img.to(device)\n",
    "            distr= model(img).cpu().numpy()\n",
    "            img_class_distr.append(distr)\n",
    "            if i > 0 and i*BATCH_SIZE % 1000 == 0:\n",
    "                logging.info(\"Val Finished {}/{}\".format(i*BATCH_SIZE, len(val_caption_dset)))\n",
    "\n",
    "\n",
    "    img_class_distr= np.concatenate(img_class_distr, axis=0)\n",
    "    print(img_class_distr.shape)\n",
    "\n",
    "    with open(val_outfilename, 'wb') as fo:\n",
    "        pickle.dump(img_class_distr, fo, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is resnet34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py:563: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Train Finished 1000/118287\n",
      "INFO:Train Finished 2000/118287\n",
      "INFO:Train Finished 3000/118287\n",
      "INFO:Train Finished 4000/118287\n",
      "INFO:Train Finished 5000/118287\n",
      "INFO:Train Finished 6000/118287\n",
      "INFO:Train Finished 7000/118287\n",
      "INFO:Train Finished 8000/118287\n",
      "INFO:Train Finished 9000/118287\n",
      "INFO:Train Finished 10000/118287\n",
      "INFO:Train Finished 11000/118287\n",
      "INFO:Train Finished 12000/118287\n",
      "INFO:Train Finished 13000/118287\n",
      "INFO:Train Finished 14000/118287\n",
      "INFO:Train Finished 15000/118287\n",
      "INFO:Train Finished 16000/118287\n",
      "INFO:Train Finished 17000/118287\n",
      "INFO:Train Finished 18000/118287\n",
      "INFO:Train Finished 19000/118287\n"
     ]
    }
   ],
   "source": [
    "model_dict= {'resnet18': resnet18,\n",
    "             'resnet34': resnet34,\n",
    "             'resnet50': resnet50,\n",
    "             'resnet101': resnet101,\n",
    "             'resnet152': resnet152,\n",
    "             'alexnet': alexnet,\n",
    "             'squeezenet': squeezenet,\n",
    "             'vgg16': vgg16,\n",
    "             'densenet': densenet,\n",
    "             'inception': inception,\n",
    "             }\n",
    "\n",
    "models_to_run= ['resnet34', 'alexnet', 'vgg16', 'densenet', 'inception', 'squeezenet']\n",
    "\n",
    "for model_name in models_to_run:\n",
    "    run_cnn_models(model_dict, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
